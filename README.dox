/**

@mainpage 15-410 Project 3

@author Prajwal Yadapadithaya (pyadapad)
@author Rohit Upadhyaya (rjupadhy)

Pebbles Kernel
--------------

The aim of this project is to implement a preemptive, multi-threaded kernel. 
We describe the design and implementation of various modules that make up the 
kernel in this README. We will describe the modules one after another with each
module explaining the various code files and the design decisions taken.

VM
--
All the code required by the VM module is present in vm/vm.c. This module is
responsible for handling the virtual memory interactions for the P3 kernel.
When the VM module is initialized, it sets up a direct mapping for the kernel
memory, sets up a special page directory (which is later used by vanish to
free resources), and enables paging. It also initializes a refernce count array
which is used to keep track of the number of tasks referring to a single
physical frame during Copy on Write (COW) operations. The VM module has 
functions to create new page directories and page tables. Whenever a new page
directory is created, the direct map is used to fill the entries required for
accessing kernel memory. There are functions implemented in the VM module to 
handle COW functionality. is_addr_cow() and handle_cow() are functions that are 
called by the page fault handler to handle COW operations. The 
clone_paging_info() function makes a copy of the entire address space which is 
used by the fork() system call. There are also functions to map various 
segments to the page tables, which are invoked when a program is being loaded 
to memory.

Interesting design decisions: We use the three extra bits present in the page
table entries for various purposes. Bit 9 is used to indicate that the page
table entry is a COW entry. Bits 10 and 11 are used by the new_pages and 
remove_pages system calls to determine the entries to be allocated and
removed. This enables us to not use extra data structures to store the
required information. 

Copy-On-Write
--------------
Forking a new process does not clone the entire virtual address space of the 
parent process. A fork() clones the page information of the parent to the child
task and makes all pages read only and sets a special bit (bit 9) in the page 
table entry to indicate that this is a writable page made readable for COW. 
This bit is inspected on page fault to determine if the page is a COW page and
hence must be cloned and made writable. We keep track of the number of 
references to each physical frame for garbage collection.

Physical Frame Allocator
------------------------
The frame allocator code present in allocator/frame_allocator.c is reponsible
for allocating physical frames. This file has functions to allocate_frame()
and deallocate_frame(). This file uses an integer array stored in kernel memory
to keep track of the physical frames that are being allocated and deallocated.
The array functions as a stack and new frames are allocated from the top of
the stack and free frames are pushed to the top of the stack. 
Along with allocating and deallocating frames, this file also provides a 
function to lock/unlock a particular frame. This is particularly useful
when the reference counts for these frames are being modified by the VM
module. We maintain a lock per physical frame in the system. While this may seem
wasteful in terms of memory, we concluded that the frame allocator is critical
and used often enough that using a giant lock would be bad for performance.

Loader
------
The loader module (loader/loader.c) is responsible for parsing the executable
file to be loaded. This file also includes the elf_load_helper() and the 
getbytes() function, which is used by the readfile system call.

Scheduler
---------
The code for the round robin scheduler for the P3 kernel is present in 
core/scheduler.c. This file implements all the functionality required for
adding threads to the runnable queue and returning the head of the queue
to the context switch code. The scheduler implements a simple round-robin
scheduling algorithm by returning the first thread in the run queue when 
requested. The scheduler may also return a sleeping thread from a separate
sleeping queue if the first thread in that queue has a wake time less than 
or equal to the current time (ticks). The scheduler is O(1) since we just
check the heads of the sleeping and running queues. 

Context Switch
--------------
The code for context switch is present in core/context.c. Context switch
works by changing the stack pointer from the current thread stack to the
stack of the thread to which the switching is being done. Context switch
is invoked from several places (timer interrupt callback function, the
kernel cond_var implementation, sleep system call, vanish system call etc). 
context_switch() invokes scheduler to get the next runnable thread. If there
is no runnable thread we run the idle task.

System calls
------------
We classify the system calls into the following categories. In each category, 
we describe the implementation and design decisions taken for each system call.
The system call handlers are all present in the folder syscalls. The files are
divided based on the category of the system call.

Life Cycle:

fork() - The code for fork system call is present in core/fork.c. When forking,
we create a new task, and make a copy of the address space by creating new page
directories and page tables. Once the new page tables are created, we mark all
the writable pages in both parent and child as COW with the help of the bit 9
in the page table entry, which is unused. Once the child task (with a single
thread) is created, we add the newly created thread to the runnable queue of
the scheduler. We also suspend execution when multiple threads from a single
task call fork() so that at any point in time, only one thread per task is
fork()ing. Forks for different tasks are executed in parallel. If any of the
steps during forking fail, we return an error code (defined in common/errors.h).
When a COW page fault is triggered by either the parent or the child, we clone the 
frame, remove the COW flag, decrement ref count and assign the cloned frame to 
the process that caused the page fault. Our page fault handler is a trap gate
and hence appropriate locking is done to avoid TOCTTOU and similar concurrency
issues.

thread_fork() - The code for thread_fork system call is present in core/fork.c.
The functionality of thread fork is similar to fork with the exception that 
there is no address space copying involved.

exec() - The code for exec system call is present in core/exec.c. During an
exec, we first try to load the new program into memory by creating new page
directories and page tables. If the program load is successful, then we
free the old paging info of the task. In case the program load is not
successful (possibly because of low memory), then we rollback the page
directory to the old page directory and return an error value.

vanish() and wait() - The code for vanish and wait system calls are present
in core/wait_vanish.c. We maintain a list of alive child tasks and dead child
tasks for every task. When a thread is vanishing, it free's all the resources
allocated to it (including the stack and the thread struct), by switching to
a static temporary stack in the kernel. We ensure that at any point in time, only
one thread is using this special stack by disbling interrupts. We call 
context_switch (which disables interrupts anyway) very soon after so preemption 
does not take a major hit. Doing so we free kernel resources much sooner.
In case the thread is the last thread in the task, then we re-parent all the 
tasks present in the alive and dead child lists to init task, remove ourselves 
from the parent task's alive child list and add to the dead child list, notify 
the parent about the death, and then continue with free'ing the thread resources 
as before using the special kernel stack. Whenever a task calls wait(), we first 
look at the dead child list, if any child is dead already, then we can reap that
child and read its exit status. If there are no dead child tasks, then we look at 
the alive child list. If there are no alive child tasks, then we can return immediately
with an error value. If there are alive child tasks, then the task calling
wait() is descheduled with the help of the condition variable abstraction
implemented in the kernel. When the task is woken up because of the exiting
of a child, it will reap the task, read its exit status and free the task
struct and other resources of the dead task.
The advantage of using a special kernel stack when a thread is vanishing is
that we can free the resources of the thread (including the stack) 
immediately and this helps in regaining the resources as quickly as 
possible. The downside of this implementation is that, since only one
thread can access the special stack at any point in time, the exiting of
the threads will be serialized. We made this design decision to ensure
that the resources are free'd as quickly as possible. We also had a design
where we would create a special "exit" stack for each thread instead of 
having just one global special stack, which would help us in running multiple
exits in parallel. But this involved keeping track of the new stacks created
and free'ing them at a later point in time (which added more complexity).
Also, the vanish() system call is slightly inefficient in the sense that 
the re-parenting of the child tasks is done atomically to avoid race conditions
when doing it. In an ideal scenario, we should serialize the re-parenting
only among the same child hierarchy, but this involves acquiring a mutex
on the parent task struct, and there is no way to prevent the parent
from vanishing before the mutex is unlocked by the child (which can cause
memory corrruption). Hence, we decided to disable interrupts for this critical
section.

Thread Management:
Most of the thread management system calls are implemented in the file
syscalls/thread_syscalls.c.

Memory Management:
The syscalls are present in syscalls/memory_syscalls.c. The new_pages and
remove_pages are implemented as part of vm/vm.c. We use the unused bits in page
table entries(bits 10 and 11) to keep track of the pages allocated via new_pages()
system call. This helps us in keeping track of the allocated pages when
a remove_pages system call is called. We do not need any additional
data structure to know the size to be free'd. We use the two bits to
indicate the start, end and middle of a new_pages allocated memory block 
which is used by the remove pages system call to free the pages.

Console I/O:
All the console related system calls are implemented in the file
syscalls/console_syscalls.c

Miscellaneous System Interaction:
readfile() system call implementation is present in syscalls/misc_syscalls.c.
It uses the getbytes() function present in loader/loader.c to read
the file contents.

Faults and Exceptions
---------------------
All the fault handlers are present in the file interrupts/fault_handler.c.
The fault handler file includes all the swexn logic and also the COW logic
needed in the page fault handler.

Locking Architecture
--------------------
Mutex: The kernel uses a blocking mutex when mutual exclusion is required.
The mutex disables interrupts and checks the value of the mutex variable going
to sleep if currently locked. The unlock functions signals the next thread that
is blocked. We also have a special version of the mutex_lock and mutex_unlock
functions which checks for the current interrupt flag but from EFLAGS and 
doesn't enable interrupts when a mutex_lock/mutex_unlock is called from
a interrupt disabled environment. Currently this is used only in condition
variables and the sfree() function in malloc wrappers (because we needed them
in these two scenarios), but they can be made generic to be used everywhere.
As a result of these two special functions, there is a slight code duplication
in the mutex implementation.

Condition variables: We extend the ideas from P2 to implement similar
condition variable abstraction. Condition variables are used only by the
wait system call and the readline system call currently

Semaphores: Semaphores are implemented using mutex and condition variables.
Currently, we do not use the semaphore implementation anywhere in our kernel.


Key Data Structures
-------------------

Generic doubly linked list:
We use a generic linked list implementation inspired by the Linux kernel 
linked list. The linked list data structure can be added to any user struct
and forms the traversal link. This allows a single struct to be on multiple
lists which can be useful at times. This special linked list helped us in
removing all the unnecessary malloc traffic as a single struct can be part
of multiple linked lists. 

A Simple map for threads:
The map we use for a quick lookup of thread control blocks is keyed on the 
thread id. The function used to determine the bucket into which a particular
thread falls into is simply the thread ID modulo the size of the map (2 pages
by default). Each bucket is then a linked list of thread blocks which have to
be linearly traversed to get the right thread control block. Since thread IDs
are assigned in increasing order, we have the nice effect of the map being 
uniformly loaded.


Limitations and Bugs
--------------------
Our vanish() implementation is slightly inefficient due to disabling interrupts
when re-parenting is being done.

*/
